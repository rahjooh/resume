<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Hadi Rahjou — Resume</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <header>
    <h1>HADI RAHJOU</h1>
    <p>
      <a href="mailto:rahjooh@gmail.com">rahjooh@gmail.com</a> |
      <a href="tel:09126780521">0912.6780521</a>
    </p>
  </header>

  <main>
    <!-- LEFT COLUMN (Now Experience) -->
    <div id="mainLeft">
      <section>
        <h2>Experience</h2>

        <h3>Bigmining <span>| Software Engineer (Go, Data, Cloud)</span></h3>
        <p><em>June 2025 – Now</em></p>
        <ul>
          <li>
            Built a real-time data ingestion system streaming high-frequency cryptocurrency
            data into Iceberg tables on AWS S3, optimized for analytical workloads.
          </li>
          <li>
            Provisioned and maintained an Amazon MSK Serverless (Kafka) cluster using
            Terraform, enabling scalable, event-driven data ingestion with automated
            infrastructure management and monitoring.
          </li>
          <li>
            Developed a cross-exchange cryptocurrency trading bot in Go, integrating
            market data streams, strategy execution, and exchange APIs to support
            low-latency order routing and multi-venue arbitrage.
          </li>
        </ul>

        <h3>Snapp <span>| Data Engineer</span></h3>
        <p><em>April 2024 – June 2025</em></p>
        <ul>
          <li>
            Led the migration of 50TB of big data infrastructure and 33 Spark legacy
            pipelines from Hadoop (YARN/HDFS) to Kubernetes and S3, enhancing scalability
            and efficiency.
          </li>
          <li>
            Led Spark cluster setup and architected end-to-end data pipelines using
            custom Helm (development), Spark APIs (deployment), and Airflow operators
            (scheduling) for Kafka sink and flattening pipelines (50+).
          </li>
          <li>
            Managed standard deployments on OKD using CI/CD and Vault with ArgoCD,
            and on VMs using Ansible.
          </li>
          <li>
            Performed server maintenance, including upgrades, storage provisioning,
            and preparing bare-metal servers to join OKD clusters.
          </li>
          <li>
            Maintained AWS infrastructure incorporating key technologies like Kafka
            and ClickHouse.
          </li>
        </ul>
      </section>
    </div>

    <!-- RIGHT COLUMN (Now Education, Skills, Links) -->
    <div id="mainRight">
      <section>
        <h2>Education</h2>
        <h3>University of Tehran</h3>
        <p>Master’s Degree in Computer Science (2016 – 2019)</p>
      </section>

      <section>
        <h2>Skills</h2>

        <h3>Big Data Engineering</h3>
        <p>Apache Spark, Hadoop Ecosystem, Kafka, Airflow, ClickHouse, Stream</p>

        <h3>Cloud</h3>
        <p>Terraform, Kubernetes / OKD, Helm, ArgoCD, Docker, AWS</p>

        <h3>Programming</h3>
        <p>Go, Python, SQL, Bash</p>

        <h3>Data Science</h3>
        <p>RFM Modeling, KMeans Clustering, Logistic Regression, Analytical Dashboards</p>

        <h3>Web & APIs</h3>
        <p>Gin, Django, FastAPI, Spring Boot</p>

        <h3>Tools</h3>
        <p>Git, Jira, Confluence, Vault</p>
      </section>

      <section>
        <h2>Links</h2>
        <p>
          Site: <a href="https://hadirahjou.ir">hadirahjou.ir</a><br />
          Github: <a href="https://github.com/rahjooh">rahjooh</a><br />
          LinkedIn: <a href="https://www.linkedin.com/in/hadirahjou/">hadirahjou</a>
        </p>
      </section>
    </div>
  </main>
</body>
</html>
